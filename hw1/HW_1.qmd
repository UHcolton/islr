---
title: "Math 6350 - Homework 1"
author: "Colton Baker"
format: 
  pdf:
    default: true
execute: 
  echo: true
  highlight: true
editor: visual
---

## Problem 1:

This question involves the use of simple linear regression on the Advertising data set. Use the lm() to fit a multiple regression model to predict sales using TV, radio and newspaper budgets. Use the summary() function to print the results.

```{r echo=FALSE}
ad_data = read.csv("~/PERSONAL/Fall 2024/Stat Learning Data mining/Chapter Notes R Files and Python Files/Ch3_Linear Regression/Advertising.csv")
```

```{r}
lm.ads <- lm(sales ~ TV + radio + newspaper, data=ad_data)
summary(lm.ads)

```

a\. We now have the equation with our estimated coefficients:

$$
\text{sales} = 2.939 + 0.046 \times\text{TV} + 
0.189 \times\text{radio} - 
0.001 \times\text{newspaper}
$$

b\. $95\%$ confidence interval:

```{r}

confint(lm.ads, level = 0.95)
```

c\) From the $p-values$ calculated in the linear regression model above, we can conclude that factors $\text{TV}$ and $\text{radio}$ are statistically significant and in turn will have a positive affect on sales. Newspaper with a $p-value > 0.05$ is not remarkable.

d\) We reject the null hypotheses that $\beta_1 = \beta_2 = 0$ which are associated with the predictors $\text{TV}$ and $\text{radio}$

e\)

```{r}
lm.ads2 <- lm(sales ~ TV + radio, data=ad_data)
summary(lm.ads2)
coef(lm.ads2)

```

The coefficients had a slight difference in the new model when rounder to 3 significant figures:

$$
\text{sales} = 2.939 + 0.046 \times\text{TV} + 
0.188 \times\text{radio}
$$

f\) Given the summary of the new model (without comparison to older model), we assess the summary statistics for goodness of fit. $\text{Multiple}\space\text{R}^2 = 0.8972$ and $\text{Adjusted}\space\text{R}^2=0.8962$ indicating around $90\%$ of the proportion of the variance is explained by our new model. The $p-value < 0.05$ suggests the features hold a high statistical significance and are a good fit. This also lets us know that the predictors used are highly likely to influence the response, sales, per unit change.

To asses the relevance of our $\text{RSE}$ score we can compare it to the standard deviation of the response variable, sales:

```{r}
rse <- 1.681
sd <- sd(ad_data$sales)

rse / sd
```

The $\text{RSE}=1.681$ which means our prediction error is about $1.681$ units. Comparing $\text{RSE}$ to the standard deviation of sales, $\frac{\text{RSE}}{\sigma(\text{sales})}=0.322$, indicates that the model has a moderately good fit in terms of $\frac{\text{residual error}} {\text{standard deviation}}$. Although this leaves around about $32\%$ of the variability in $\text{sales}$ unexplained by our model.

In conclusion, yes this model is a good fit.

## Problem 2:

I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression

$$Y=\beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon$$

a\) The cubic model's training $\text{RSS}$ would be less than or equal to any model with a lesser degree which includes the linear model. It would fit the data more closely, minimizing the training $\text{RSS}$ but include larger amounts of noise i.e. over fit the data. This is not good. We want the model to learn the truth as best as possible while minimizing its capacity to capture unexplained phenomena. This is where bias-variance trade off should be assessed.

b\) For the test $\text{RSS}$, our linear model would generalize well to unseen data since it most likely represents our linearity of out dataset more honestly, matching the true relationship between $X$ and $Y$, thus minimizing our test $\text{RSS}$**.** The cubic model would unlikely generalize well on unseen data due to a higher variance and more parameter's fitting the noise. This would lead to higher test $\text{RSS}$.

c\) The cubic model's training $\text{RSS}$ would be less than or equal to the linear model's. In general, the higher the degree the more capacity the model has to minimize the training $\text{RSS}$ due to higher flexibility.

d\) If the true relationship between $X$ and $Y$ is non-linear, then there is not enough information to know whether or not a linear or cubic model would generalize better without experimentation. If the data is non-linear but close to linear then the linear model may generalize better and the cubic would most likely over fit and thus have a higher test $\text{RSS}$.

```{r}
# AN example using linear data. Did not do the non-linear data example.
# this is mostly for my own review purposes
set.seed(6350)
#generating a linear relationsihp for experiment
n <- 100 # number of observations
X <- rnorm(n, mean=0, sd=1) # feature
B_0 <- runif(1, min=-1, max=1) #intercept
B_1 <- runif(1, min=-1, max=1) #slope
noise <- rnorm(n, mean=0, sd=1) #noise term i.e. epsilon
Y <- B_0 + B_1 * X + noise


#split, train, test
set.seed(6350)
training_index <- sample(1:n, size=0.8 * n) #80 samples for training
test_index <- setdiff(1:n, training_index) # 20 samples for testing

X_train <- X[training_index] 
Y_train <- Y[training_index]
X_test <- X[test_index]
Y_test <- Y[test_index]

# linear model
lm.fit <- lm(Y_train ~ X_train)

#cubic model, raw -> These are the standard polynomial terms
cubic.fit <- lm(Y_train ~ poly(X_train, degree = 3, raw = TRUE))

anova.on.models <- anova(lm.fit, cubic.fit) #p-value = 0.4418362 > 0.05

#training RSS for both models on linear data
lm.train.preds <- predict(lm.fit, newdata = data.frame(X_train = X_train))
lm.train.rss <- sum((Y_train - lm.train.preds)^2)

cubic.train.preds <- predict(cubic.fit, newdata = data.frame(X_train = X_train))
cubic.train.rss <- sum((Y_train - cubic.train.preds)^2)

rss.difference <- cubic.train.rss - lm.train.rss
rss.difference # neg value -> cubic rss is less than lm rss
```

## Problem 3:

This question involves the use of simple linear regression on the Auto data set.

```{r}
suppressWarnings(library(ISLR2))

data("Auto")
auto.lm <- lm(mpg ~ horsepower, data = Auto)

summary(auto.lm)
```

i\. Yes. The coefficient $\beta_1=-0.1578$ along with its associated $p-value$ tells us that there a significant relationship between mpg and horsepower.

ii\. To assess the strength of the relationship we can peek at the $\text{R}^2=0.6059$ which tells us that $60.59\%$ of the variability in mpg can be explained by horsepower implying a moderate relationship between them.

iii\. negative

iv\. In the code below the predict function is called with varying interval types. The first being the prediction and the second being the confidence. In both variable outputs, the predicted value of mpg given horsepower of 98 is shown under fit.

```{r}
pred.interval <- predict(auto.lm, data.frame(horsepower = 98), interval = "prediction")
conf.interval <- predict(auto.lm, data.frame(horsepower = 98), interval = "confidence")

pred.interval
conf.interval
```

b\)

```{r}
plot(Auto$horsepower, Auto$mpg, xlab = "Horsepower", 
                                ylab = "MPG", 
                                main = "Regression of MPG on Horsepower")
abline(auto.lm, col = "red")
```

c\)

```{r}
plot(auto.lm)
```

**Comments:**

From the residuals vs fitted plot, we see that the data may not be as linear as assumed, suggesting a linear model may not be the best fit.

The Q-Q plot suggests normality within the distribution allowing us to retain this assumption.

Scale-Location suggests homoscedasticity, or constant variance, among the residuals but we still need to assess significance of outliers.

From our Leverage plot, there are outliers with high leverage and residual values, particulary observations 18 (or 19 hard to tell from plot) and 117. These data points could influence the model's fit so these observations should be analyzed further and dealt with appropriately.

## Problem 4:

In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.

```{r}
# a)
set.seed(6350)
x <- rnorm(100, mean = 0, sd = 1) 
# b)
eps <- rnorm(100, mean = 0, sd = sqrt(0.25))
# c)
y <- -1 + 0.5*x + eps 
length(y)
```

c\) The length of $y$ is 100, $\beta_0=-1$ and $\beta_1=0.5$.

```{r}
# d)
scatter.smooth(x, y, xlab = "Simulated Feature: X",
                     ylab = "Simulated Linear Model: Y",
                     main = "X and Y Relationship")
cor.xy <- cor(x, y)
cor.xy
```

**Comments:**

The $\text{Cor}(x, y)=0.6617345$ indicating a moderately strong positive relationship between $x$ and $y$. This metric is further supported by the scatter plot which shows a mostly positive trend.

The data seems to be mostly linear suggesting a linear regression model would be useful for prediction.

There are outliers but this is expected from the random noise we generated.

The variance of residuals seems to remain relatively constant throughout the data which may allow us to assume homoscedatiscity for our linear model (view Q-Q plot below).

e\)

```{r}
lm.xy <- lm(y ~ x)
summary(lm.xy)
plot(lm.xy)
```

**Comment:**

The linear model obtained by regressing y onto x , seems to be a good fit for our simulated truth.

Comparing $\beta_0$ and $\beta_1$ to $\hat{\beta_0}$ and $\hat{\beta_1}$ we can see that

$$
|\beta_0 - \hat{\beta_0}|=0.01056
$$

and

$$
|\beta_1 - \hat{\beta_1}|=0.00209
$$

Which tells us that there *is* a minimal amount of error due to randomly generated noise. Since the distance between the coefficients is relatively small, this model to make predictions with relatively small error. This is an assumption based solely on our coefficients and further analysis is needed to assess the legitimacy of said assumption.

Taking a look at the 95% confidence interval (see below), t-test, and our p-value as from the summary above, we can see that the $t-value$ is relatively far from 0 and $p-value$ is extremely small indicating a significance between $x$ and $y$.

```{r}
confint(lm.xy)
```

The confidence interval tells us that with 95% confidence, our *true* $\beta$ 's lie between the ranges

$$
\beta_0 \in [-1.0927638,\space -0.8861134]\\
$$

and

$$
\beta_1 \in [0.3880525,\space0.6161212]
$$

which is expected since we simulated the truth.

f\)

```{r}
plot(x, y, xlab = "Simulated Feature: X",
           ylab = "Simulated Linear Model: Y",
           main = "X and Y Relationship")
abline(a = -1, b = 0.5, col = "green")
# had hard coded a and b but now use coef function for (a, b). Great find!
abline(a = coef(lm.xy)[1], b = coef(lm.xy)[2], col = "purple") 
legend("topleft", cex = 0.5, 
                  legend = c("Population Regression Line", "Least Squares Line"),
                  col = c("purple", "green"),
                  pch = 4, 
                  bty = "n", 
                  title = "Legend")
```

g\)

```{r}
lm.poly.xy <- lm(y ~ x + I(x^2))
summary(lm.poly.xy)
plot(lm.poly.xy)
```

**Explanation:**

After fitting the a polynomial regression model that predicts $y$ using $x$ and $x^2$, we can take a look at the model's statistics. Just from the $p-value = 0.313 > 0.05$ alone we can conclude that the model's performance is not optimized.

h\) **Model with reduced noise**:

```{r suppressWarnings}
# a)
set.seed(6350)
x1 <- rnorm(100, mean = 0, sd = 1) 
# b) WITH REDUCED NOISE
eps1 <- rnorm(100, mean = 0, sd = sqrt(0.01))
# c)
y1 <- -1 + 0.5*x1 + eps1 
length(y1)

# d)
scatter.smooth(x1, y1, xlab = "Simulated Feature: X",
                       ylab = "Simulated Linear Model: Y",
                       main = "X and Y Relationship")
# e)
lm.xy1 <- lm(y1 ~ x1)
summary(lm.xy1)

# f)
plot(x1, y1, xlab = "Simulated Feature: X1",
             ylab = "Simulated Linear Model: Y1",
             main = "X1 and Y1 Relationship")
abline(a = -1, b = 0.5, col = "blue")
abline(a = coef(lm.xy)[1], b = coef(lm.xy)[2], col = c("red")) # glad I found coef()
legend("topleft", cex = 0.5, 
                  legend = c("Population Regression Line", "Least Squares Line"),
                  col = c("blue", "red"),
                  pch = 6, 
                  title = "Legend")
```

**Description of Results from Model with Reduced Noise:**

Decreasing the $Var(ϵ)=0.25$ to $Var(ϵ)=0.01$ results in the data points "tightening" around the regression lines. It is worth noting that this is due to the **decrease** in randomly generated noise.

Comparing $\{\beta_0, \beta_1\}$ to $\{\hat{\beta_0},\hat{\beta_1}\}$,

$|\beta_0−\hat{\beta_0}|=0.00211$

and

$|\beta_1−\hat{\beta_1}|=0.001669456$

Since the distance between the true values and estimated values of $\beta$ is **less than** our original model, there is **less** **residual error.** Therefore predictions *should be* relatively close to the truth.

**Model with Increased Noise:**

```{r}
# a)
set.seed(6350)
x2 <- rnorm(100, mean = 0, sd = 1) 
# b) WITH REDUCED NOISE
eps2 <- rnorm(100, mean = 0, sd = sqrt(2))
# c)
y2 <- -1 + 0.5*x2 + eps2 
length(y2)

# d)
scatter.smooth(x2, y2, xlab = "Simulated Feature: X2",
                       ylab = "Simulated Linear Model: Y2",
                       main = "X2 and Y2 Relationship")
# e)
lm.xy2 <- lm(y2 ~ x2)
summary(lm.xy2)

# f)
plot(x2, y2, xlab = "Simulated Feature: X",
             ylab = "Simulated Linear Model: Y",
             main = "X and Y Relationship")
abline(a = -1, b = 0.5, col = "orange")
abline(a = coef(lm.xy2)[1], b = coef(lm.xy2)[2], col = "cyan") # glad I found coef()
legend("topleft", cex = 0.5, 
                  legend = c("Population Regression Line", "Least Squares Line"),
                  col = c("orange", "cyan"),
                  pch = 10, 
                  title = "Legend")
```

**Description of Results from Model with Increased Noise:**

Increasing the $Var(ϵ)=0.25$ to $Var(ϵ)=0.01$ results in the data points "loosening" around the regression lines. It is worth noting that this is due to the **increase** in randomly generated noise.

Comparing $\{\beta_0, \beta_1\}$ to $\{\hat{\beta_0},\hat{\beta_1}\}$,

$|\beta_0−\hat{\beta_0}|=0.02987209$

and

$|\beta_1−\hat{\beta_1}|=0.005902417$

Since the distance between the true values and estimated values of $\beta$ is **greater than** our original model, there is **greater** **residual error.** Therefore, predictions may be far less accurate than in our previous models.

```{r}
CI.lm.xy <- confint(lm.xy)
CI.lm.xy1 <- confint(lm.xy1)
CI.lm.xy2 <- confint(lm.xy2)

#for s's and g's

CI.df <- data.frame(
  Model = c("Original B_0","Original B_1", 
            "Inc. Noise B_0", "Inc. Noise B_1",
            "Dec. Noise B_0","Dec. Noise B_1"),
  Lower_Bound = c(CI.lm.xy[1], CI.lm.xy2[1], CI.lm.xy2[1]),
  Upper_Bound = c(CI.lm.xy[2], CI.lm.xy2[2], CI.lm.xy2[2])
)
  
CI.df
```

**Comment:** It is worth noting that similarities in confidence intervals are present due to the fact that the underlying truth is the same for each. The variance in the values for each range is directly correlated to adjustments made in the noise term, i.e. $\epsilon$.

**Lesson:** When the truth is known it is simple to discern it from estimates. But as we saw it is often the case that before it can reach us, it picks up noise along the way. We can try our best to decipher it, but the amount of noise present effects the amount of truth efficiently received. The distance between what we know and what we want to understand lies in the interference.
